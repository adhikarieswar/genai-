# -*- coding: utf-8 -*-
"""pdf reader correct

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BKc7IF2z1u8dTIOC1i4sdGmU5Yl1-Q_L
"""

!pip install langchain
!pip install openai
!pip install tiktoken
!pip install faiss-cpu

!pip install PyPDF2
!pip  install langchain_community

from IPython import get_ipython
from IPython.display import display
from PyPDF2 import PdfReader
from langchain.embeddings.huggingface import HuggingFaceEmbeddings   # Changed from OpenAIEmbeddings
from langchain.text_splitter import CharacterTextSplitter
from langchain.vectorstores import FAISS
from langchain.chat_models import ChatOpenAI
from langchain.chains.question_answering import load_qa_chain
import os

# Set your Hugging Face API key if needed (in case you're using a private model)
#os.environ['HUGGINGFACE_API_KEY'] = 'YOUR_HUGGINGFACE_API_KEY'  # Uncomment if you need it.

# Load the PDF


# Split the text into chunks

# Create embeddings and store in FAISS using Hugging Face embeddings


# Load QA chain

os.environ['groq_api_key']='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'

pdfreader = PdfReader('/content/new_important_resume_avinash.pdf')
raw_text = ""
for i, page in enumerate(pdfreader.pages):
    content = page.extract_text()
    if content:
        raw_text += content

text_splitter = CharacterTextSplitter(separator='\n', chunk_size=800, chunk_overlap=200, length_function=len)
texts = text_splitter.split_text(raw_text)

llm= ChatOpenAI(
        openai_api_base="https://api.groq.com/openai/v1",
        openai_api_key=os.environ['groq_api_key'],
        model_name="llama3-8b-8192",
        temperature=0,
        max_tokens=1000,
    )

embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")  # Hugging Face embedding model
document_search = FAISS.from_texts(texts, embeddings)

chain = load_qa_chain(llm, chain_type="stuff")

query = "what is education of him"

docs = document_search.similarity_search(query)

# Get the answer

answer = chain.run(input_documents=docs, question=query)
print(answer)