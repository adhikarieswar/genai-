# -*- coding: utf-8 -*-
"""project on langraph 1

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1m0nI9117K11KB8wOKMH4hTU-3TajSYOF
"""

!pip install langgraph langchain-openai langchain-community beautifulsoup4 python-dotenv

# Import required libraries
from langgraph.graph import Graph
from langchain_core.messages import HumanMessage, AIMessage
from langchain_core.prompts import ChatPromptTemplate
from langchain_community.tools import DuckDuckGoSearchResults
from langchain_community.document_loaders import WebBaseLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.chains.summarize import load_summarize_chain
from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import StrOutputParser
from langchain_openai import ChatOpenAI
from typing import TypedDict, List, Dict, Any
!pip install -U duckduckgo-search

#

# Initialize our LLM (using a small CPU-friendly model)
llm=ChatOpenAI(
        openai_api_base="https://api.groq.com/openai/v1",
        openai_api_key="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",
        model_name="deepseek-r1-distill-llama-70b",
        temperature=0.5,
        max_tokens=1000,
        streaming=True
    )
class AgentState(TypedDict):
    input: str
    research_plan: List[str]
    gathered_info: List[Dict[str, Any]]
    report: str

# Create search tool with proper configuration
search = DuckDuckGoSearchResults(backend="text")

import re  # Add this at the top with other imports

def extract_urls(search_results):
    try:
        if isinstance(search_results, str):
            url_pattern = r'https?://[^\s]+'
            urls = re.findall(url_pattern, search_results)
            return [url for url in urls if not url.endswith('.pdf')][:2]
        return []
    except Exception as e:
        print(f"Error extracting URLs: {str(e)}")
        return []

def research_planner(state: AgentState):
    print("\n🔍 Research Planner working...")
    prompt = ChatPromptTemplate.from_template(
        "Create 3-5 research questions about: {input}\n"
        "Format as a numbered list."
    )
    chain = prompt | llm | StrOutputParser()
    questions = chain.invoke({"input": state["input"]})
    return {
        **state,  # Preserve existing state
        "research_plan": [q.strip() for q in questions.split("\n") if q.strip() and q[0].isdigit()]
    }

def information_gatherer(state: AgentState):
    print("\n🌐 Information Gatherer working...")
    gathered_info = []

    for question in state["research_plan"]:
        print(f"  Researching: {question}")
        try:
            search_results = search.run(question)
            urls = extract_urls(search_results)

            for url in urls:
                try:
                    loader = WebBaseLoader(url)
                    docs = loader.load()

                    text_splitter = RecursiveCharacterTextSplitter(
                        chunk_size=3000,
                        chunk_overlap=300
                    )
                    splits = text_splitter.split_documents(docs)

                    summarize_chain = load_summarize_chain(llm, chain_type="map_reduce")
                    summary = summarize_chain.run(splits[:2])

                    gathered_info.append({
                        "question": question,
                        "source": url,
                        "summary": summary
                    })
                except Exception as e:
                    print(f"    ⚠️ Error processing {url}: {str(e)}")
                    gathered_info.append({
                        "question": question,
                        "source": url,
                        "error": str(e)
                    })

        except Exception as e:
            print(f"  ⚠️ Failed to research question: {question} - {str(e)}")
            gathered_info.append({
                "question": question,
                "error": f"Research failed: {str(e)}"
            })

    return {**state, "gathered_info": gathered_info}  # Preserve existing state

def report_writer(state: AgentState):
    print("\n✍️ Report Writer working...")

    context = []
    for item in state["gathered_info"]:
        if "summary" in item:
            context.append(
                f"### {item['question']}\n"
                f"**Source:** {item['source']}\n"
                f"**Summary:** {item['summary']}\n"
            )
        else:
            context.append(
                f"### {item['question']}\n"
                f"⚠️ Research failed: {item.get('error', 'Unknown error')}\n"
            )

    prompt = ChatPromptTemplate.from_template(
        "Write a 500-700 word report on: {input}\n\n"
        "Research Context:\n{context}\n\n"
        "Include sections: Introduction, Findings, Conclusion. Cite sources."
    )

    chain = prompt | llm | StrOutputParser()
    report = chain.invoke({
        "input": state["input"],
        "context": "\n".join(context)
    })

    return {**state, "report": report}  # Preserve existing state

workflow = Graph()
workflow.add_node("planner", research_planner)
workflow.add_node("gatherer", information_gatherer)
workflow.add_node("writer", report_writer)

workflow.add_edge("planner", "gatherer")
workflow.add_edge("gatherer", "writer")

workflow.set_entry_point("planner")
workflow.set_finish_point("writer")

research_app = workflow.compile()

# Define your research topic
research_topic = "The impact of large language models on education"

# Execute the workflow
results = research_app.invoke({"input": research_topic})

# Display the final report
print("\n\n===== FINAL RESEARCH REPORT =====")
print(results["report"])

# (Optional) Save to a file
with open("research_report.txt", "w") as f:
    f.write(results["report"])

!pip install moviepy langchain-openai gtts pillow

from gtts import gTTS

# Convert report to speech
tts = gTTS(results["report"], lang='en', slow=False)
tts.save("narration.mp3")

from IPython.display import Audio, display

# Play the audio automatically (autoplay=True)
display(Audio("narration.mp3", autoplay=False))  # Shows player controls

from moviepy.editor import *
from PIL import Image, ImageDraw, ImageFont
import warnings
warnings.filterwarnings("ignore", category=SyntaxWarning)


# Create slides with key points
def create_slide(text, output_path):
    img = Image.new('RGB', (1280, 720), color=(15, 23, 42))
    draw = ImageDraw.Draw(img)

    # Use a available font (Colab has Arial)
    try:
        font = ImageFont.truetype("Arial.ttf", 40)
    except:
        font = ImageFont.load_default()

    draw.text((100, 300), text, font=font, fill=(255, 255, 255))
    img.save(output_path)

# Split report into slides
key_points = results["report"].split("\n\n")[:5]  # First 5 paragraphs
for i, point in enumerate(key_points):
    create_slide(point, f"slide_{i}.jpg")



def create_slide(text, image_path, output_path):
    img = Image.new('RGB', (1280, 720), color=(15, 23, 42))
    draw = ImageDraw.Draw(img)

    # Try custom font
    try:
        font = ImageFont.truetype("Arial.ttf", 36)
    except:
        font = ImageFont.load_default()

    # Insert image (resized)
    if image_path:
        insert_img = Image.open(image_path).resize((400, 400))
        img.paste(insert_img, (100, 160))  # paste image on left side

    # Wrap and draw text
    import textwrap
    wrapped_text = textwrap.fill(text, width=40)
    draw.text((550, 200), wrapped_text, font=font, fill=(255, 255, 255))  # text on right side

    img.save(output_path)

image_path = "/content/llm image.jpeg"

for i, point in enumerate(key_points):
    create_slide(point, image_path, f"slide_{i}.jpg")

from moviepy.editor import concatenate_videoclips, AudioFileClip

# Concatenate video clips
video = concatenate_videoclips(clips, method="compose")

# Load audio narration
audio = AudioFileClip("narration.mp3")

# Sync durations if necessary
if audio.duration > video.duration:
    audio = audio.subclip(0, video.duration)
else:
    video = video.set_duration(audio.duration)

# Combine video with audio
final_video = video.set_audio(audio)



from moviepy.editor import ImageClip, AudioFileClip, concatenate_videoclips
from IPython.display import HTML
from base64 import b64encode

# List of slide image filenames
slide_files = [f"slide_{i}.jpg" for i in range(len(key_points))]  # or 5 if you have 5 slides

# STEP 1: Create video clips from each slide
clips = []
for slide in slide_files:
    clip = ImageClip(slide).set_duration(5).crossfadein(1)
    clips.append(clip)

# STEP 2: Concatenate all slide clips into one video
video = concatenate_videoclips(clips, method="compose")

# STEP 3: Load narration audio
audio = AudioFileClip("narration.mp3")

# STEP 4: Sync durations
if audio.duration > video.duration:
    audio = audio.subclip(0, video.duration)
else:
    video = video.set_duration(audio.duration)

# STEP 5: Add audio to video
final_video = video.set_audio(audio)

# STEP 6: Export the final video
final_video.write_videofile("full_presentation.mp4", fps=24, codec="libx264", audio_codec="aac")

mp4_path = "full_presentation.mp4"
mp4 = open(mp4_path, 'rb').read()
data_url = "data:video/mp4;base64," + b64encode(mp4).decode()

HTML(f"""
<video width=700 controls>
    <source src="{data_url}" type="video/mp4">
</video>
""")